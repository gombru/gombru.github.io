---
layout: page
title: PhD Thesis Defence
permalink: /phd_defence/
order: 5
---

### Exploiting the Interplay between Visual and Textual Data for Scene Interpretation

<div class="imgcap">
	<div style="display:inline-block">
	<img src="/assets/phd_defence.png" height="330">
	</div>
</div>

- **Date & time**: 08/10/2020 - 12:00h (Spanish time, GMT+2)
- [**Live Stream**](http://www.cvc.uab.es/?p=7127)
- [**Slides**](https://docs.google.com/presentation/d/1LUnzsnMyzRCg23v7q_uMM9bcpubHAxovpUdsz4vYZH0/edit?usp=sharing)
- [**Thesis**](https://drive.google.com/file/d/1ljxxzqbqq2kZPUjV1_-aQOOG6omAg69y/view?usp=sharing)

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vR279Isz0FZ5o43C-kK2RilqIHkZzg6p4uZddrSJBp0D8fWKqCQbawoWj5_hvMvtxsW-VhclryBAys5/embed?start=false&loop=false&delayms=3000" frameborder="0" width="768" height="455" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

**Committee:**

Dr. Josep Lladós (Centre de Visió per Computador, Universitat Autònoma de Barcelona)

Dr. Diane Larlus (Naver Labs Europe)

Dr. Francesc Moreno (Universitat Politècnica de Cataunya, Institut de Robòtica i Informàtica Industrial)

**Thesis Director:**

Dr. Dimosthenis Karatzas (Centre de Visió per Computador, Universitat Autònoma de Barcelona)

**Thesis Co-Directors:**

Dr. Lluis Gómez (Centre de Visió per Computador, Universitat Autònoma de Barcelona)

Dr. Jaume Gibert (Eurecat)

**Abstract:**

Machine learning experimentation under controlled scenarios and standard datasets is necessary to compare algorithms performance by evaluating all of them in the same setup. However, experimentation on how those algorithms perform on unconstrained data and applied tasks to solve real world problems is also a must to ascertain how that research can contribute to our society.

In this dissertation we experiment with the latest computer vision and natural language processing algorithms applying them to multimodal scene interpretation. Particularly, we research on how image and text understanding can be jointly exploited to address real world problems, focusing on learning from Social Media data.

We address several tasks that involve image and textual information, discuss their characteristics and offer our experimentation conclusions. First, we work on detection of scene text in images. Then, we work with Social Media posts, exploiting the captions associated to images as supervision to learn visual features, which we apply to multimodal semantic image retrieval. Subsequently, we work with geolocated Social Media images with associated tags, experimenting on how to use the tags as supervision, on location sensitive image retrieval and on exploiting location information for image tagging. Finally, we work on a specific classification problem of Social Media publications consisting on an image and a text: Multimodal hate speech classification.